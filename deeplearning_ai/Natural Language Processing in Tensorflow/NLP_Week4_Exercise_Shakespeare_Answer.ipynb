{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"NLP_Week4_Exercise_Shakespeare_Answer.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"zX4Kg8DUTKWO"},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOwsuGQQY9OL","executionInfo":{"status":"ok","timestamp":1617790294978,"user_tz":-420,"elapsed":3173,"user":{"displayName":"Agung Prabowo M3142818","photoUrl":"","userId":"03605229693661895840"}}},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import regularizers\n","import tensorflow.keras.utils as ku \n","import numpy as np "],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRnDnCW-Z7qv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617790295881,"user_tz":-420,"elapsed":4068,"user":{"displayName":"Agung Prabowo M3142818","photoUrl":"","userId":"03605229693661895840"}},"outputId":"ecfcb227-b2d4-49fb-b9fe-85b57f059055"},"source":["tokenizer = Tokenizer()\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n","    -O /tmp/sonnets.txt\n","data = open('/tmp/sonnets.txt').read()\n","\n","corpus = data.lower().split(\"\\n\")\n","\n","\n","tokenizer.fit_on_texts(corpus)\n","total_words = len(tokenizer.word_index) + 1\n","\n","# create input sequences using list of tokens\n","input_sequences = []\n","for line in corpus:\n","\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n","\tfor i in range(1, len(token_list)):\n","\t\tn_gram_sequence = token_list[:i+1]\n","\t\tinput_sequences.append(n_gram_sequence)\n","\n","\n","# pad sequences \n","max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","\n","# create predictors and label\n","predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n","\n","label = ku.to_categorical(label, num_classes=total_words)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2021-04-07 10:11:37--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.128, 108.177.126.128, 108.177.127.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 93578 (91K) [text/plain]\n","Saving to: ‘/tmp/sonnets.txt’\n","\n","\r/tmp/sonnets.txt      0%[                    ]       0  --.-KB/s               \r/tmp/sonnets.txt    100%[===================>]  91.38K  --.-KB/s    in 0.002s  \n","\n","2021-04-07 10:11:37 (50.0 MB/s) - ‘/tmp/sonnets.txt’ saved [93578/93578]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w9vH8Y59ajYL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617790296942,"user_tz":-420,"elapsed":5114,"user":{"displayName":"Agung Prabowo M3142818","photoUrl":"","userId":"03605229693661895840"}},"outputId":"0a742487-128c-48ce-ddc2-96601791744a"},"source":["model = Sequential()\n","model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n","model.add(Bidirectional(LSTM(150, return_sequences = True)))\n","model.add(Dropout(0.2))\n","model.add(LSTM(100))\n","model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n","model.add(Dense(total_words, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 10, 100)           321100    \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 10, 300)           301200    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 10, 300)           0         \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 100)               160400    \n","_________________________________________________________________\n","dense (Dense)                (None, 1605)              162105    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3211)              5156866   \n","=================================================================\n","Total params: 6,101,671\n","Trainable params: 6,101,671\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AIg2f1HBxqof","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8e110b77-a8a8-4268-900c-a3cb8b8062d3"},"source":[" history = model.fit(predictors, label, epochs=100, verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","484/484 [==============================] - 62s 118ms/step - loss: 7.3286 - accuracy: 0.0172\n","Epoch 2/100\n","484/484 [==============================] - 56s 115ms/step - loss: 6.4880 - accuracy: 0.0231\n","Epoch 3/100\n","484/484 [==============================] - 56s 115ms/step - loss: 6.3779 - accuracy: 0.0209\n","Epoch 4/100\n","484/484 [==============================] - 57s 118ms/step - loss: 6.2657 - accuracy: 0.0276\n","Epoch 5/100\n","484/484 [==============================] - 57s 118ms/step - loss: 6.1794 - accuracy: 0.0334\n","Epoch 6/100\n","484/484 [==============================] - 55s 114ms/step - loss: 6.0822 - accuracy: 0.0340\n","Epoch 7/100\n","484/484 [==============================] - 55s 114ms/step - loss: 6.0149 - accuracy: 0.0353\n","Epoch 8/100\n","484/484 [==============================] - 55s 113ms/step - loss: 5.9540 - accuracy: 0.0393\n","Epoch 9/100\n","484/484 [==============================] - 55s 113ms/step - loss: 5.8475 - accuracy: 0.0482\n","Epoch 10/100\n","484/484 [==============================] - 55s 114ms/step - loss: 5.7287 - accuracy: 0.0560\n","Epoch 11/100\n","484/484 [==============================] - 55s 114ms/step - loss: 5.6221 - accuracy: 0.0623\n","Epoch 12/100\n","484/484 [==============================] - 56s 115ms/step - loss: 5.5431 - accuracy: 0.0650\n","Epoch 13/100\n","484/484 [==============================] - 55s 114ms/step - loss: 5.4314 - accuracy: 0.0718\n","Epoch 14/100\n","484/484 [==============================] - 55s 113ms/step - loss: 5.2861 - accuracy: 0.0871\n","Epoch 15/100\n","484/484 [==============================] - 55s 114ms/step - loss: 5.2231 - accuracy: 0.0887\n","Epoch 16/100\n","484/484 [==============================] - 55s 114ms/step - loss: 5.0790 - accuracy: 0.0929\n","Epoch 17/100\n","484/484 [==============================] - 57s 118ms/step - loss: 4.9923 - accuracy: 0.1021\n","Epoch 18/100\n","484/484 [==============================] - 56s 115ms/step - loss: 4.9001 - accuracy: 0.1080\n","Epoch 19/100\n","484/484 [==============================] - 55s 115ms/step - loss: 4.8066 - accuracy: 0.1164\n","Epoch 20/100\n","484/484 [==============================] - 57s 117ms/step - loss: 4.7301 - accuracy: 0.1251\n","Epoch 21/100\n","484/484 [==============================] - 55s 114ms/step - loss: 4.6065 - accuracy: 0.1415\n","Epoch 22/100\n","484/484 [==============================] - 56s 116ms/step - loss: 4.5010 - accuracy: 0.1461\n","Epoch 23/100\n","484/484 [==============================] - 56s 116ms/step - loss: 4.3935 - accuracy: 0.1608\n","Epoch 24/100\n","484/484 [==============================] - 56s 116ms/step - loss: 4.3091 - accuracy: 0.1703\n","Epoch 25/100\n","484/484 [==============================] - 54s 112ms/step - loss: 4.1954 - accuracy: 0.1852\n","Epoch 26/100\n","484/484 [==============================] - 57s 117ms/step - loss: 4.0561 - accuracy: 0.2061\n","Epoch 27/100\n","484/484 [==============================] - 56s 116ms/step - loss: 4.0192 - accuracy: 0.2075\n","Epoch 28/100\n","484/484 [==============================] - 56s 115ms/step - loss: 3.8864 - accuracy: 0.2287\n","Epoch 29/100\n","484/484 [==============================] - 55s 114ms/step - loss: 3.7969 - accuracy: 0.2464\n","Epoch 30/100\n","484/484 [==============================] - 55s 113ms/step - loss: 3.7022 - accuracy: 0.2584\n","Epoch 31/100\n","484/484 [==============================] - 56s 117ms/step - loss: 3.5758 - accuracy: 0.2908\n","Epoch 32/100\n","484/484 [==============================] - 56s 116ms/step - loss: 3.4931 - accuracy: 0.3026\n","Epoch 33/100\n","484/484 [==============================] - 56s 116ms/step - loss: 3.4116 - accuracy: 0.3271\n","Epoch 34/100\n","484/484 [==============================] - 54s 113ms/step - loss: 3.3097 - accuracy: 0.3482\n","Epoch 35/100\n","484/484 [==============================] - 56s 115ms/step - loss: 3.2286 - accuracy: 0.3649\n","Epoch 36/100\n","484/484 [==============================] - 55s 115ms/step - loss: 3.1415 - accuracy: 0.3839\n","Epoch 37/100\n","484/484 [==============================] - 54s 112ms/step - loss: 3.0833 - accuracy: 0.3966\n","Epoch 38/100\n","484/484 [==============================] - 54s 112ms/step - loss: 2.9723 - accuracy: 0.4296\n","Epoch 39/100\n","484/484 [==============================] - 56s 115ms/step - loss: 2.9047 - accuracy: 0.4467\n","Epoch 40/100\n","484/484 [==============================] - 55s 114ms/step - loss: 2.8655 - accuracy: 0.4531\n","Epoch 41/100\n","484/484 [==============================] - 55s 114ms/step - loss: 2.8124 - accuracy: 0.4668\n","Epoch 42/100\n","484/484 [==============================] - 56s 116ms/step - loss: 2.7161 - accuracy: 0.4868\n","Epoch 43/100\n","484/484 [==============================] - 56s 115ms/step - loss: 2.6899 - accuracy: 0.4898\n","Epoch 44/100\n","484/484 [==============================] - 57s 117ms/step - loss: 2.5955 - accuracy: 0.5109\n","Epoch 45/100\n","484/484 [==============================] - 57s 118ms/step - loss: 2.5492 - accuracy: 0.5240\n","Epoch 46/100\n","484/484 [==============================] - 56s 115ms/step - loss: 2.4760 - accuracy: 0.5393\n","Epoch 47/100\n","484/484 [==============================] - 55s 114ms/step - loss: 2.4260 - accuracy: 0.5544\n","Epoch 48/100\n","484/484 [==============================] - 57s 117ms/step - loss: 2.3798 - accuracy: 0.5710\n","Epoch 49/100\n","484/484 [==============================] - 56s 116ms/step - loss: 2.3145 - accuracy: 0.5783\n","Epoch 50/100\n","468/484 [============================>.] - ETA: 1s - loss: 2.2894 - accuracy: 0.5850"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1fXTEO3GJ282"},"source":["import matplotlib.pyplot as plt\n","acc = history.history['accuracy']\n","loss = history.history['loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'b', label='Training accuracy')\n","plt.title('Training accuracy')\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'b', label='Training Loss')\n","plt.title('Training loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Vc6PHgxa6Hm"},"source":["seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n","next_words = 100\n","  \n","for _ in range(next_words):\n","\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n","\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","\tpredicted = model.predict_classes(token_list, verbose=0)\n","\toutput_word = \"\"\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == predicted:\n","\t\t\toutput_word = word\n","\t\t\tbreak\n","\tseed_text += \" \" + output_word\n","print(seed_text)"],"execution_count":null,"outputs":[]}]}